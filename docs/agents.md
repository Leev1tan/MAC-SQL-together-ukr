# Documentation for core/agents.py

This module defines a multi-agent system designed to convert natural language questions into executable SQL queries for SQLite databases, leveraging a Large Language Model (LLM) for complex reasoning tasks. The system follows a pipeline structure: `Selector` -> `Decomposer` -> `Refiner`.

## Core Concepts

*   **Agents**: Independent components each responsible for a specific sub-task in the query processing pipeline. They communicate by passing a `message` dictionary containing relevant information.
*   **LLM Interaction**: The `Selector`, `Decomposer`, and `Refiner` agents interact with an external LLM (like Llama 3 via Together.ai, as per your stack) using a common API function (`LLM_API_FUC`). This function is dynamically imported from either `core.api` or `core.llm`.
*   **Pipeline Flow**: A user query (along with database ID and optional evidence) starts at the `Selector`, moves to the `Decomposer`, and finally to the `Refiner` for execution and potential correction before being finalized.

## Agent Details

### 1. `BaseAgent`

*   **Purpose**: This is an abstract base class using Python's `abc` module. It serves as a blueprint for all other agents in this file.
*   **Functionality**: It defines a standard structure, requiring any class that inherits from it (like `Selector`, `Decomposer`, `Refiner`) to implement a `talk(self, message: dict)` method. This ensures all agents have a consistent way to receive and process messages.

### 2. `Selector` Agent

*   **Purpose**: To prepare the necessary database schema information for the LLM. It identifies the relevant tables and columns for a given user query, especially for large databases.
*   **Functionality**:
    *   **Initialization (`__init__`)**: Loads metadata about all available databases from a `tables.json` file (like table names, column counts) and stores it. Optionally (`lazy=False`), it can pre-load detailed schema info for all databases.
    *   **Schema Loading (`_load_single_db_info`)**: Connects to a specific SQLite database file (`.sqlite`), reads table structures (`PRAGMA table_info`), identifies primary and foreign keys, and fetches sample values for non-key columns (`_get_unique_column_values_str`, `_get_value_examples_str`). This helps the LLM understand the data better.
    *   **Schema Formatting (`_get_db_desc_str`, `_build_bird_table_schema_list_str`)**: Converts the raw schema information into a structured text format suitable for the LLM prompt.
    *   **Pruning (`_is_need_prune`, `_prune`)**: Checks if the database schema is large (based on column counts). If it is, and if pruning is enabled (`without_selector=False`), it calls the LLM with the query and the full schema. The LLM's response (parsed as JSON) indicates which tables/columns are most relevant. This step reduces the complexity for the next agent.
    *   **Communication (`talk`)**: Receives a message containing `db_id`, `query`, and `evidence`. It loads/retrieves the schema, decides whether to prune, potentially calls the LLM for pruning, formats the final schema description (`desc_str`) and foreign key info (`fk_str`), adds them to the message, and forwards the message to the `Decomposer`.

### 3. `Decomposer` Agent

*   **Purpose**: To translate the user's natural language question into an SQL query, using the schema provided by the `Selector`.
*   **Functionality**:
    *   **Initialization (`__init__`)**: Sets up the agent, noting the dataset (`bird` or `spider`) as prompt templates might differ.
    *   **LLM Call (`call_llm`, `talk`)**: Constructs a detailed prompt containing the user query, evidence (if any), the formatted schema description (`desc_str`), and foreign key information (`fk_str`). It sends this prompt to the LLM. It uses different prompt templates (`decompose_template_bird` or `decompose_template_spider`) depending on the dataset.
    *   **SQL Parsing (`talk`)**: Extracts the SQL query from the LLM's response using `parse_sql_from_string`. It also stores the LLM's reasoning steps (`qa_pairs`).
    *   **Communication (`talk`)**: Adds the generated `final_sql` and `qa_pairs` to the message dictionary and forwards it to the `Refiner`.

### 4. `Refiner` Agent

*   **Purpose**: To execute the SQL query generated by the `Decomposer` against the actual database, validate the result, and attempt to fix the SQL using the LLM if errors occur.
*   **Functionality**:
    *   **Initialization (`__init__`)**: Stores the path to the database files and the dataset name.
    *   **SQL Execution (`_execute_sql`)**: Connects to the specified database (`db_id`) and runs the received SQL query. It uses `func_set_timeout` to prevent queries from running indefinitely (timeout set to 120 seconds). It captures any `sqlite3` errors or other exceptions.
    *   **Validation (`_is_need_refine`)**: Checks the execution result. Refinement is triggered if:
        *   An SQL execution error occurred.
        *   The query ran successfully but returned no data (`len(data) == 0`).
        *   The query returned data containing `None` values (specifically checked for the BIRD dataset, suggesting stricter data quality requirements there).
    *   **Refinement (`_refine`)**: If validation fails, it constructs a new prompt for the LLM. This prompt includes the original query, schema, the faulty SQL, and the specific error message (`sqlite_error`, `exception_class`). It asks the LLM to correct the SQL.
    *   **Communication (`talk`)**: Orchestrates the execute-validate-refine loop.
        *   Receives the message with the SQL (`pred`).
        *   Executes the SQL.
        *   If execution is successful and passes validation (or if it times out), it considers the SQL final, increments `try_times`, and passes the message to the `SYSTEM_NAME`.
        *   If refinement is needed, it calls `_refine` to get a `new_sql`, updates the `pred` in the message with this new SQL, sets `fixed = True`, increments `try_times`, and sends the message back *to itself* (`REFINER_NAME`) to try executing the corrected query. This loop continues until the SQL works, times out, or potentially hits a maximum try limit (though max limit isn't explicitly coded here, just tracking `try_times`).

## Dependencies

*   `core.utils`: For helper functions like parsing JSON/SQL, loading files, etc.
*   `core.const`: Likely contains constants like agent names (`SELECTOR_NAME`, etc.) and prompt templates (`selector_template`, etc.).
*   `core.api` / `core.llm`: Provides the `safe_call_llm` function for interacting with the LLM.
*   `func_timeout`: To limit SQL execution time.
*   Standard libraries: `sqlite3`, `os`, `json`, `abc`, `time`, `sys`, `copy`, `typing`, `re`.
*   Third-party libraries: `pandas`, `tqdm`, `tiktoken` (used in commented-out code for token counting).
