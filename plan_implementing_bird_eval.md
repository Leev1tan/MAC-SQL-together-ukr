**Strategy:**

*   Leverage the existing multi-agent architecture (`EnhancedChatManager`, `bird_extensions`).
*   Create dedicated scripts for running and evaluating BIRD tests, similar to the Spider workflow.
*   Integrate with `evaluate_metrics.py` for standardized metric calculation (EX, VES), adapting as needed for BIRD. Exact Match (EM) might require BIRD-specific logic or tools if they exist.
*   Ensure consistent logging and output formats.

**Proposed File Structure:**

```
├── core/
│   ├── bird_extensions.py         # (Exists) Enhanced agents for BIRD
│   └── ... (other core files)
├── MAC-SQL/
│   └── data/
│       └── minidev/
│           └── MINIDEV/
│               ├── dev_databases/     # (Exists) BIRD databases (SQLite)
│               ├── mini_dev_sqlite.json # (Exists) BIRD dev questions/SQL
│               └── dev_tables.json    # (Exists) BIRD table schemas for dev
├── logs/                          # (Exists/Create) Directory for logs
│   ├── bird_evaluation.log
│   └── agent_test_bird.log
├── output/                        # (Exists/Create) Directory for results
│   └── bird_agent_results.json
├── docs/                          # (Exists) Documentation folder
│   └── ...
├── evaluate_metrics.py            # (Exists) Script for calculating EM, EX, VES
├── test_macsql_agent_bird.py      # (Exists/Enhance) Detailed test script for BIRD agents
├── run_bird_evaluation.py         # (Create) Top-level script to run BIRD evaluation
└── ... (other project files in root)
```

**Detailed Tasks & Code Organisation:**

**Task 1: Enhance `test_macsql_agent_bird.py`**

*   **Objective:** Make this script functionally equivalent to `test_macsql_agent_spider.py`, but tailored for BIRD.
*   **Code Organisation:**
    *   Add argument parsing (`argparse`) for `--samples`, `--visualize`, `--output`, etc.
    *   Implement `find_bird_data()` function (similar to the Spider version) to locate the BIRD dataset (`dev.json`, `dev_databases`, `tables.json`).
    *   Implement `load_bird_queries()` function (can likely reuse `load_bird_subset` logic).
    *   Modify the main loop:
        *   Initialize `EnhancedChatManager` correctly for `dataset_name="bird"`.
        *   Prepare the message dictionary for each BIRD query (including `db_id`, `query`, `evidence`, `ground_truth`).
        *   Call `chat_manager.start()` (or loop through `_chat_single_round`).
        *   **Implement BIRD Execution Check:** This is critical.
            *   Create a function `execute_and_compare_bird_queries(pred_sql, gold_sql, db_id, db_path)` similar to the Spider version.
            *   This function needs to connect to the correct BIRD database in `data/bird/dev_databases/`.
            *   Execute both `pred_sql` and `gold_sql`.
            *   Compare the results. BIRD's official evaluation might have specific rules or a script for comparison (worth checking the BIRD benchmark documentation/repository). If not, a simple comparison of fetched results (like in `execute_and_compare_queries`) can be a starting point.
            *   Return `True` for match, `False` otherwise, and potentially the results.
        *   Store detailed results for each query (predicted SQL, gold SQL, execution match status) in a list.
    *   Integrate optional visualization using `core/tracking` and `core/visualization` (mirroring the Spider script's integration).
    *   Save the results list to the specified output JSON file.

**Task 2: Create `run_bird_evaluation.py`**

*   **Objective:** Create a top-level script to orchestrate the BIRD evaluation.
*   **Code Organisation:**
    *   Mirror the structure of `run_spider_evaluation.py`.
    *   Implement argument parsing (`--samples`, `--visualize` etc.).
    *   Call `find_bird_data()`.
    *   Implement `verify_database_structure()` for BIRD paths.
    *   Implement `run_evaluation()` function:
        *   Set environment variables if needed.
        *   Execute `test_macsql_agent_bird.py` as a subprocess (`subprocess.run`), passing arguments like `--samples`.
    *   Implement `analyze_results()` function:
        *   Read the results JSON file generated by `test_macsql_agent_bird.py`.
        *   Calculate overall execution accuracy.
        *   **(Future):** Call functions from `evaluate_metrics.py` to calculate EM and VES for BIRD, once those are adapted.
        *   Print a summary report to the console.
    *   Define the `main()` function to orchestrate these steps.

**Task 3: Adapt/Verify `evaluate_metrics.py` for BIRD**

*   **Objective:** Ensure the metrics calculation works correctly for BIRD data.
*   **Code Organisation:**
    *   **Execution Accuracy (EX):** The `compute_execution_accuracy` function should work directly if Task 1 correctly populates the `execution_match` field in the results. The underlying execution comparison might need BIRD-specific logic (see Task 1). The `evaluate_mac_sql_execution_accuracy` function *specifically* uses Spider's `eval_exec_match` and won't work directly for BIRD unless BIRD uses the exact same evaluation script, which is unlikely.
    *   **Valid Efficiency Score (VES):** The `compute_valid_efficiency_score` logic (timing queries, geometric mean) is generally applicable. It depends on having accurate EX results and the `iterated_execute_sql` function working correctly with BIRD databases.
    *   **Exact Match (EM):** The `compute_exact_match` function currently uses Spider's official `Evaluator`. This **will not work** for BIRD. You need to:
        *   Check if the BIRD benchmark provides its own official evaluation script or library for EM. If yes, integrate that.
        *   If not, you'll need to define what constitutes an "Exact Match" for BIRD (e.g., canonical string comparison, abstract syntax tree comparison tailored for BIRD's potential SQL variations) and implement the logic. This could be complex.

**Task 4: Documentation**

*   **Objective:** Document the new scripts and the BIRD evaluation process.
*   **Tasks:**
    *   Add docstrings to new/modified functions and classes.
    *   Update the main project `README.md` to explain how to run the BIRD evaluation (`python run_bird_evaluation.py --samples ...`).
    *   Potentially create a `docs/bird_evaluation.md` explaining the specifics of the BIRD evaluation process and metrics.

**Considerations:**

*   **BIRD Official Evaluation:** Prioritize finding and using BIRD's official evaluation script/methodology if it exists, especially for EX and EM, to ensure comparable results with other research.
*   **Schema Differences:** BIRD schemas and SQL might have different characteristics than Spider. The `bird_extensions.py` should handle schema formatting, but evaluation logic (especially EM) needs to account for this.
*   **Evidence:** BIRD includes 'evidence' which Spider doesn't. The evaluation needs to ensure this is handled correctly during agent processing but doesn't interfere with metrics calculation (which compares SQL).

This plan provides a clear path to establishing a robust BIRD evaluation pipeline comparable to the existing Spider one. I can help create the file structure or stub out the new files (`run_bird_evaluation.py`) if you'd like to proceed.
